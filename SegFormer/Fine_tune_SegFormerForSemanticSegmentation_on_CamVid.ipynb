{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Notebook: SegFormer for Semantic Segmentation on CamVid\n","\n","ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€CamVidãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€Hugging faceã«ã‚ã‚‹SegFormerã‚’ä½¿ã£ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è©¦ã—ã¾ã™\n","\n","ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä½œæˆã«ã‚ãŸã‚Šã€ä¸‹è¨˜ã‚’å‚è€ƒã«ã•ã›ã¦é ‚ãã¾ã—ãŸğŸ¤—\n","\n","- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n","- https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb\n"],"metadata":{"id":"-wGmNpuJhUA9"}},{"cell_type":"markdown","source":["# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n","fine-tuneã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹google driveã‚’ãƒã‚¦ãƒ³ãƒˆã—ã¾ã™\n","\n","Hugging faceã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«(Transformers, Datasets)ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™"],"metadata":{"id":"HckN1MHDjT27"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install transformers\n","!pip install datasets"],"metadata":{"id":"R3lBRK5D5Xur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰\n","ä»¥å‰ã€[SAMã®fine-tuningã‚’ä½¿ã£ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ¤œè¨¼](https://github.com/yh0sh/MyStudy/blob/main/SAM/Fine_tune_SAM_(segment_anything)_on_CamVid.ipynb)ã§ä½œæˆã—ãŸCamVidã‚’ä¿®æ­£ã—ãŸç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging faceã®datasetsã«ç™»éŒ²ã—ã¾ã—ãŸ\n","\n","https://huggingface.co/datasets/yh0sh/resized_camvid_annot_car\n","\n","- ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ç”»åƒã¨maskãƒ‡ãƒ¼ã‚¿ã‚’320\\*480 => 256\\*256ã¸ãƒªã‚µã‚¤ã‚º\n","- maskãƒ‡ãƒ¼ã‚¿ã¯carã®é ˜åŸŸã¯1ã€carä»¥å¤–ã®é ˜åŸŸã¯0ã«ãªã‚‹ã‚ˆã†ã«åŠ å·¥\n","\n","ä»Šå›ã¯ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã™"],"metadata":{"id":"-psRNzbWbnPW"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"yh0sh/resized_camvid_annot_car\")"],"metadata":{"id":"0uqUHrzh47UA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def show_mask(mask, ax, random_color=False):\n","    if random_color:\n","        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n","    else:\n","        color = np.array([30/255, 144/255, 255/255, 0.6])\n","    h, w = mask.shape[-2:]\n","    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n","    ax.imshow(mask_image)\n","\n","sample = dataset[\"train\"][10]\n","\n","# ç”»åƒãƒ‡ãƒ¼ã‚¿\n","image = sample[\"image\"]\n","\n","# maskãƒ‡ãƒ¼ã‚¿\n","# NOTE maskãƒ‡ãƒ¼ã‚¿ã¯grayscaleã«å¤‰æ›å¾Œã€æœ€å¤§å€¤ã§å‰²ã‚‹ã“ã¨ã§{0, 1}ã®å€¤åŸŸã«æ¨™æº–åŒ–ã™ã‚‹\n","mask = np.array(sample[\"annot\"].convert(\"L\")).astype(\"float64\")\n","mask /= np.nanmax(mask)\n","\n","fig, axes = plt.subplots()\n","axes.imshow(np.array(image))\n","show_mask(np.array(mask), axes)\n","axes.title.set_text(f\"Ground truth mask\")\n","axes.axis(\"off\")\n","plt.show()"],"metadata":{"id":"n1LA3ywS7qpL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PyTorch Datasetã®ä½œæˆ\n","PyTorch Datasetã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ä½œã‚Šã¾ã™"],"metadata":{"id":"WboY52-gs-4V"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import os\n","from PIL import Image\n","\n","class SemanticSegmentationDataset(Dataset):\n","\n","    def __init__(self, dataset, feature_extractor, data_label):\n","        assert data_label in [\"train\", \"validation\", \"test\"]\n","        self.dataset = dataset[data_label]\n","        self.feature_extractor = feature_extractor\n","        self.data_label = data_label\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","\n","        image = self.dataset[idx][\"image\"]\n","\n","        # NOTE maskãƒ‡ãƒ¼ã‚¿ã¯grayscaleã«å¤‰æ›å¾Œã€æœ€å¤§å€¤ã§å‰²ã‚‹ã“ã¨ã§{0, 1}ã®å€¤åŸŸã«æ¨™æº–åŒ–ã™ã‚‹\n","        mask = np.array(self.dataset[idx][\"annot\"].convert(\"L\")).astype(\"float64\")\n","        mask /= np.nanmax(mask)\n","        segmentation_map = mask\n","\n","        # randomly crop + pad both image and segmentation map to same size\n","        encoded_inputs = self.feature_extractor(image, segmentation_map,\n","                                                return_tensors=\"pt\")#, input_data_format=\"channels_last\")\n","\n","        for k,v in encoded_inputs.items():\n","          encoded_inputs[k].squeeze_() # remove batch dimension\n","\n","        return encoded_inputs"],"metadata":{"id":"5K0yxwjIs0nz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FeatureExtractorã®èª­ã¿è¾¼ã¿\n","SegFormerã®FeatureExtractorã‚’èª­ã¿è¾¼ã¿ã¾ã™"],"metadata":{"id":"xZwn-1WRxNiP"}},{"cell_type":"code","source":["from transformers import SegformerImageProcessor\n","\n","feature_extractor = SegformerImageProcessor(do_reduce_labels=False) # ä»Šå›ã¯2å€¤åˆ†é¡ãªã®ã§ do_reduce_labels = False ã¨ã™ã‚‹ã“ã¨ã§ã€\n","                                                                    # unknownã¯é™¤å¤–ã›ãšã€label = 0ã¨ã—ã¦ä½¿ç”¨\n","\n","train_dataset = SemanticSegmentationDataset(dataset, feature_extractor, \"train\")\n","valid_dataset = SemanticSegmentationDataset(dataset, feature_extractor, \"validation\")\n","test_dataset = SemanticSegmentationDataset(dataset, feature_extractor, \"test\")\n","\n","print(\"Number of training examples:\", len(train_dataset))\n","print(\"Number of validation examples:\", len(valid_dataset))\n","print(\"Number of testing examples:\", len(test_dataset))"],"metadata":{"id":"8EhOpGYJxdFx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PyTorch DataLoaderã®ä½œæˆ\n","å…ˆã«ä½œæˆã—ãŸPyTorch Datasetã‚’å…ƒã«DataLoaderã‚’ä½œæˆã—ã¾ã™"],"metadata":{"id":"Km4-gFqjyNDY"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from collections import Counter\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","# SegFormerImageProcessorã«ã‚ˆã‚Šã€256*256 => 512*512ã«ãƒªã‚µã‚¤ã‚º\n","for batch in train_dataloader:\n","    for k, v in batch.items():\n","        print (k, np.array(v).shape)\n","\n","    # carã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã•ã‚ŒãŸãƒ”ã‚¯ã‚»ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª\n","    # label = 0 (= unknown)ã‚’maskã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ\n","    mask = (batch[\"labels\"] != 0)[0]\n","\n","    plt.figure()\n","    plt.imshow(mask, cmap=\"gray\")\n","    break"],"metadata":{"id":"yZRw7cEGyO_V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelã®èª­ã¿è¾¼ã¿"],"metadata":{"id":"koBNkTcWk9ae"}},{"cell_type":"markdown","source":["SegFormerã®äº‹å‰å­¦ç¿’ã¨ã—ã¦ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã™\n","\n","ä»Šå›ã¯ã€carã®ã¿ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ãŸã‚ã€äºŒå€¤ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã„ã¾ã™\n","\n","å‰å‡¦ç†ã¨ã—ã¦ã€ä»¥ä¸‹ã‚’è¡Œã„ã¾ã—ãŸ(â€»)\n","\n"," - 2å€¤å•é¡Œã¨ã—ã¦æ‰±ã„ã€label = 0 (unknown), 1 (car) ã¨ã™ã‚‹\n","      - ADE20Kã®ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€huggingfaceã®datasetsã«ã‚ã‚‹[label-files](https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json)ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n"," - 0: unknownã¯ç„¡è¦–ã›ãšå­¦ç¿’å¯¾è±¡ã¨ã—ã¦æ‰±ã†\n","      - [mean_iou](https://huggingface.co/spaces/evaluate-metric/mean_iou)ã‚’ç”¨ã„ã¦è©•ä¾¡ã™ã‚‹éš›ã€ignore_indexã«unknownã®ãƒ©ãƒ™ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§è©•ä¾¡ã‹ã‚‰é™¤å¤–ã™ã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹\n","      - ã—ã‹ã—ã€unknownã‚’ç„¡è¦–ã™ã‚‹ã¨ã€carã®ãƒ©ãƒ™ãƒ«ã®ã¤ã„ãŸé ˜åŸŸ(ground truth)å†…ã§ã—ã‹è©•ä¾¡ã—ãªããªã‚‹ãŸã‚ã€å­¦ç¿’ã§ã¯lossã‚’æ­£ã—ãè¨ˆç®—ã§ããªããªã‚‹\n"," - 0: unknownã‚’255ã¨ã—ãªã„\n","      - do_reduce_labels = Trueã¨ã—ãªã„\n","\n","â€» ä½•åº¦ã‹å€¤ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆã¦å®Ÿæ–½ã—ãŸä¸Šã§ã®èª¿æŸ»çµæœã¨ãªã‚Šã¾ã™ãŒã€è§£é‡ˆãŒé–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\n","\n","èª¤è§£ã—ã¦ã„ã‚‹ã¨ã“ã‚ãŒã‚ã‚Œã°ã€ã”æŒ‡æ‘˜é ‚ã‘ã‚‹ã¨åŠ©ã‹ã‚Šã¾ã™ğŸ™"],"metadata":{"id":"rSIJO6BNI5B4"}},{"cell_type":"code","source":["from transformers import SegformerForSemanticSegmentation\n","\n","# NOTE: ADE20Kã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹labelãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã™\n","#import json\n","#from huggingface_hub import cached_download, hf_hub_url, hf_hub_download\n","#\n","## load id2label mapping from a JSON on the hub\n","#repo_id = \"huggingface/label-files\"\n","#filename = \"ade20k-id2label.json\"\n","#id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type=\"dataset\")), \"r\"))\n","#id2label = {int(k): v for k, v in id2label.items()}\n","#label2id = {v: k for k, v in id2label.items()}\n","\n","id2label = {0: \"unknown\", 1: \"car\"}\n","label2id = {v: k for k, v in id2label.items()}\n","\n","# define model\n","model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n","                                                         num_labels=2,\n","                                                         id2label=id2label,\n","                                                         label2id=label2id,\n",")"],"metadata":{"id":"lIxsJ4GZk7_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"],"metadata":{"id":"kEn8lb1LoWgt"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score\n","from tqdm.notebook import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","\n","from collections import Counter\n","\n","num_epochs = 100\n","\n","# define optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n","# move model to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = \"cpu\"\n","model.to(device)\n","\n","# å­¦ç¿’çµæœã‚’ä¿å­˜ã™ã‚‹\n","save_dir = \"/content/drive/MyDrive/colab/SegFormer\"   # æ‰€æœ›ã®å ´æ‰€ã«å¤‰æ›´ã—ã¦ä¸‹ã•ã„\n","losses_path = f\"{save_dir}/loss_ft_segformer_camvid-b0.csv\"\n","columns = [\"epoch\", \"loss_train\", \"loss_eval\"]\n","df_loss = pd.DataFrame([], columns=columns)\n","start_epoch = 0\n","best_loss = np.inf\n","\n","# é€”ä¸­ã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã™ã‚‹å ´åˆã€ä¿å­˜ã—ãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€\n","save_path = f\"{save_dir}/fine_tune_segformer_camvid-b0.pth\"\n","if os.path.exists(save_path):\n","    if device == \"cpu\":\n","        checkpoint = torch.load(save_path, map_location=torch.device(\"cpu\"))\n","    else:\n","        checkpoint = torch.load(save_path)\n","    model.load_state_dict(checkpoint)\n","\n","    df_loss = pd.read_csv(losses_path)\n","    start_epoch = df_loss[\"epoch\"].values[-1] + 1\n","    best_loss = df_loss[\"loss_eval\"].min()\n","\n","for epoch in range(start_epoch, num_epochs):  # loop over the dataset multiple times\n","    print(f'## EPOCH: {epoch} best_loss_eval: {best_loss}')\n","\n","    # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º\n","    model.train()\n","    epoch_losses = []\n","\n","    print (f\"train data {len(train_dataloader)}...\")\n","    for idx, batch in enumerate(tqdm(train_dataloader)):\n","        # get the inputs;\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","        loss, logits = outputs.loss, outputs.logits\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_losses.append(loss.item())\n","    loss_train = np.mean(epoch_losses)\n","    print(f'Mean loss (train): {loss_train}')\n","\n","    # è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚º\n","    model.eval()\n","    with torch.no_grad():\n","        epoch_losses = []\n","\n","        print (f\"valid data {len(valid_dataloader)}...\")\n","        for idx, batch in enumerate(tqdm(valid_dataloader)):\n","            # get the inputs;\n","            pixel_values = batch[\"pixel_values\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            # forward + backward + optimize\n","            outputs = model(pixel_values=pixel_values, labels=labels)\n","            loss, logits = outputs.loss, outputs.logits\n","\n","            epoch_losses.append(loss.item())\n","\n","        loss_eval = np.mean(epoch_losses)\n","        print(f'Mean loss (valid): {loss_eval}')\n","\n","        # å­¦ç¿’çµæœä¿å­˜\n","        result = {\n","            \"epoch\": epoch,\n","            \"loss_train\": loss_train,\n","            \"loss_eval\": loss_eval\n","        }\n","        df_loss = pd.concat([df_loss, pd.DataFrame([result])], axis=0)\n","        df_loss.to_csv(losses_path, index=False)\n","\n","        # è©•ä¾¡å€¤æ›´æ–°\n","        if np.mean(epoch_losses) < best_loss:\n","            best_loss = np.mean(epoch_losses)\n","            torch.save(model.state_dict(), save_path)\n","            print(\"Model saved!\")\n"],"metadata":{"id":"L2KWD3COoV80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# å­¦ç¿’çµæœã®å¯è¦–åŒ–"],"metadata":{"id":"3b2UEKQhIlAM"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure()\n","plt.plot(df_loss[\"epoch\"], df_loss[\"loss_train\"], label=\"train\")\n","plt.plot(df_loss[\"epoch\"], df_loss[\"loss_eval\"], label=\"eval\")\n","plt.title(\"Learning result\")\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","plt.grid(True)\n","plt.legend(loc=\"best\")\n","plt.show()"],"metadata":{"id":"ss9s3LBAInN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["epochæ•°ãŒå¢—ãˆã‚‹ã«é€£ã‚Œã¦æå¤±ã¯ä½ä¸‹ã—ã¦ãŠã‚Šã€å­¦ç¿’ã¯é †èª¿ã«è¡Œã‚ã‚ŒãŸã‚ˆã†ã§ã™"],"metadata":{"id":"-A89l_G6KZlx"}},{"cell_type":"markdown","source":["# æ¨å®šçµæœ\n","å­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã„ã€å­¦ç¿’ãŒæ­£ã—ãè¡Œã‚ã‚ŒãŸã‹ç¢ºèªã—ã¾ã™\n","\n","äºˆæ¸¬ãƒã‚¹ã‚¯å€¤ã€æ­£è§£ãƒã‚¹ã‚¯å€¤ã‚’ç¢ºèªã—ã¾ã™"],"metadata":{"id":"gB-pHDduIxmN"}},{"cell_type":"code","source":["# è©•ä¾¡ç”¨ã«ä¹±æ•°ã‚’å›ºå®šã™ã‚‹\n","seed = 42\n","torch.manual_seed(seed)\n","\n","# GPUã‚’ä½¿ã†å ´åˆã«ã¯ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚‚è¿½åŠ \n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","idx = 10\n","data = dataset[\"train\"][idx]\n","image = data[\"image\"]\n","\n","mask = np.array(sample[\"annot\"].convert(\"L\")).astype(\"float64\")\n","mask /= np.nanmax(mask)\n","\n","encoded_inputs = feature_extractor(image, return_tensors=\"pt\")#, input_data_format=\"channels_last\")\n","pixel_values = encoded_inputs.pixel_values.to(device)\n","\n","model.eval()\n","\n","# forward pass\n","with torch.no_grad():\n","    outputs = model(pixel_values=pixel_values)\n","\n","# logits are of shape (batch_size, num_labels, height/4, width/4)\n","logits = outputs.logits.cpu()\n","print(logits.shape)\n","\n","# First, rescale logits to original image size\n","upsampled_logits = nn.functional.interpolate(logits,\n","                size=image.size[::-1], # (height, width)\n","                mode='bilinear',\n","                align_corners=False)\n","\n","seg = upsampled_logits.argmax(dim=1)[0]\n","\n","sf_seg = seg.detach().cpu().numpy()\n","\n","fig, axes = plt.subplots()\n","axes.imshow(np.array(image))\n","show_mask(np.array(sf_seg), axes)\n","axes.title.set_text(f\"Predicted mask\")\n","axes.axis(\"off\")\n","plt.show()\n","\n","fig, axes = plt.subplots()\n","axes.imshow(np.array(image))\n","ground_truth_seg = np.array(mask)\n","show_mask(ground_truth_seg, axes)\n","axes.title.set_text(f\"Ground truth mask\")\n","axes.axis(\"off\")"],"metadata":{"id":"LhRT3xItIxNI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ä¸Šè¨˜çµæœã‚’å…ƒã«IoUã‚’è¨ˆç®—ã—ã¾ã™"],"metadata":{"id":"gglpJsHQ9QF2"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_iou(segmentation1, segmentation2):\n","    intersection = np.logical_and(segmentation1, segmentation2)\n","    union = np.logical_or(segmentation1, segmentation2)\n","    iou = np.sum(intersection) / np.sum(union)\n","    return iou\n","\n","# IoUã‚’è¨ˆç®—\n","iou_score = calculate_iou(sf_seg.astype(bool), ground_truth_seg.astype(bool))\n","print(f\"IoU: {iou_score}\")"],"metadata":{"id":"qGL0RiV49PIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["æ¬¡ã«ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿å…¨ã¦ã®IoUã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã€è©•ä¾¡ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®IoUå¹³å‡å€¤ã‚’ãã‚Œãã‚Œç¢ºèªã—ã¾ã™"],"metadata":{"id":"toC7kIJf9YVD"}},{"cell_type":"code","source":["# è©•ä¾¡ç”¨ã«ä¹±æ•°ã‚’å›ºå®šã™ã‚‹\n","seed = 42\n","torch.manual_seed(seed)\n","\n","# GPUã‚’ä½¿ã†å ´åˆã«ã¯ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚‚è¿½åŠ \n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","labels = [\"train\", \"validation\", \"test\"]\n","iou_scores_dict = {label: [] for label in labels}\n","\n","iou_scores_path = \"{save_dir}/iou_scores_{label}_ft_sf_camvid-b0.csv\"\n","for label in labels:\n","    if os.path.exists(iou_scores_path.format(save_dir = save_dir, label = label)):\n","        continue\n","\n","    print (f\"###  {label}\")\n","    for idx, inputs in tqdm(enumerate(dataset[label]), desc=f\"{label} data {len(dataset[label])}\"):\n","\n","        image_id = inputs[\"image_id\"]\n","        image = inputs[\"image\"]\n","\n","        mask = np.array(inputs[\"annot\"].convert(\"L\")).astype(\"float64\")\n","        mask /= np.nanmax(mask)\n","\n","        encoded_inputs = feature_extractor(image, return_tensors=\"pt\")#, input_data_format=\"channels_last\")\n","        pixel_values = encoded_inputs.pixel_values.to(device)\n","\n","        model.eval()\n","\n","        # forward pass\n","        with torch.no_grad():\n","            outputs = model(pixel_values=pixel_values)\n","\n","        # logits are of shape (batch_size, num_labels, height/4, width/4)\n","        logits = outputs.logits.cpu()\n","\n","        # First, rescale logits to original image size\n","        upsampled_logits = nn.functional.interpolate(logits,\n","                        size=image.size[::-1], # (height, width)\n","                        mode='bilinear',\n","                        align_corners=False)\n","\n","        seg = upsampled_logits.argmax(dim=1)[0]\n","\n","        sf_seg = seg.detach().cpu().numpy()\n","\n","        ground_truth_seg = np.array(mask)\n","\n","        iou_score = calculate_iou(sf_seg.astype(bool), ground_truth_seg.astype(bool))\n","        iou_scores_dict[label].append([image_id, iou_score])\n","    print (f\"mean: {np.nanmean(np.array(iou_scores_dict[label])[:,1].astype(float), axis=0)}\")\n","    print (f\"std: {np.nanstd(np.array(iou_scores_dict[label])[:,1].astype(float), axis=0)}\")\n","\n","    data = np.array(iou_scores_dict[label])\n","    pd.DataFrame(data).to_csv(iou_scores_path.format(save_dir = save_dir, label = label),\n","                              header=None, index=False)"],"metadata":{"id":"sMByJVuC9ZUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["means = []\n","stds = []\n","nums = []\n","for label in labels:\n","    df = pd.read_csv(iou_scores_path.format(save_dir = save_dir, label = label), header=None)\n","    means.append(df.values[:,1].mean())\n","    stds.append(df.values[:,1].std())\n","    nums.append(df.values[:,1].size)\n","\n","plt.figure()\n","plt.errorbar(np.arange(len(labels)), means, yerr=stds)\n","plt.xticks(np.arange(len(labels)), [f\"{label} (N={num})\" for label, num in zip(labels, nums)])\n","plt.ylabel(\"IoU\")\n","plt.title(\"IoU Score\")\n","plt.grid(True)"],"metadata":{"id":"sYgJdg3j_EYE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯91%ç¨‹åº¦ã€è©•ä¾¡ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã¯75%ç¨‹åº¦ã¨ä¸­ã€…ã®é«˜ç²¾åº¦ã¨ãªã‚Šã¾ã—ãŸ\n","\n","ã“ã‚Œã¯ [SAMã‚’fine-tuningã—ãŸçµæœ](https://github.com/yh0sh/MyStudy/blob/main/SAM/Fine_tune_SAM_(segment_anything)_on_CamVid.ipynb)ã«æ¯”ã¹ã¦ã€è©•ä¾¡ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯3-5%ç¨‹åº¦ä½ã„å¹³å‡å€¤ã¨ãªã£ã¦ã„ã¾ã™\n","\n","ã¾ãŸã€æ¨™æº–åå·®ã¯5%ç¨‹åº¦å¤§ãããªã£ã¦ãŠã‚Šã€ãƒ‡ãƒ¼ã‚¿å˜ä½ã§ã®ç²¾åº¦ã®å·®ãŒå¤§ãããªã£ã¦ã„ã¾ã™\n","\n","æœ€é«˜ã¨æœ€ä½ã®IoUã‚¹ã‚³ã‚¢ã¨ãªã£ãŸç”»åƒã‚’å‡ºåŠ›ã—ã¦ã€å…¥åŠ›ç”»åƒã®ç‰¹å¾´ã‚„å‚¾å‘ã‚’ç›®è¦–ç¢ºèªã—ã¦ç²¾åº¦ã¸ã®å½±éŸ¿ã‚’ç¢ºèªã—ã¾ã™"],"metadata":{"id":"PJto0TWfBgST"}},{"cell_type":"code","source":["# è©•ä¾¡ç”¨ã«ä¹±æ•°ã‚’å›ºå®šã™ã‚‹\n","seed = 42\n","torch.manual_seed(seed)\n","\n","# GPUã‚’ä½¿ã†å ´åˆã«ã¯ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚‚è¿½åŠ \n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","def show_box(box, ax):\n","    x0, y0 = box[0], box[1]\n","    w, h = box[2] - box[0], box[3] - box[1]\n","    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n","\n","from collections import Counter\n","\n","for label in labels:\n","    df = pd.read_csv(iou_scores_path.format(save_dir = save_dir, label = label), header=None)\n","    image_ids = df.values[:,0]\n","    iou_scores = df.values[:,1]\n","\n","    max_score_idx = np.argmax(iou_scores)\n","    min_score_idx = np.argmin(iou_scores)\n","\n","    ds_dict = {inputs[\"image_id\"]: inputs for inputs in dataset[label]}\n","    for score_idx in [max_score_idx, min_score_idx]:\n","        iou_score = iou_scores[score_idx]\n","        image_id = image_ids[score_idx]\n","\n","        inputs = ds_dict[image_id]\n","\n","        image = inputs[\"image\"]\n","\n","        mask = np.array(inputs[\"annot\"].convert(\"L\")).astype(\"float64\")\n","        mask /= np.nanmax(mask)\n","\n","        encoded_inputs = feature_extractor(image, return_tensors=\"pt\")#, input_data_format=\"channels_last\")\n","        pixel_values = encoded_inputs.pixel_values.to(device)\n","\n","        model.eval()\n","\n","        # forward pass\n","        with torch.no_grad():\n","            outputs = model(pixel_values=pixel_values)\n","\n","        # logits are of shape (batch_size, num_labels, height/4, width/4)\n","        logits = outputs.logits.cpu()\n","\n","        # First, rescale logits to original image size\n","        upsampled_logits = nn.functional.interpolate(logits,\n","                        size=image.size[::-1], # (height, width)\n","                        mode='bilinear',\n","                        align_corners=False)\n","\n","        seg = upsampled_logits.argmax(dim=1)[0]\n","\n","        sf_seg = seg.detach().cpu().numpy()\n","\n","        ground_truth_seg = np.array(mask)\n","\n","        plt.figure()\n","        plt.imshow(image)\n","        plt.title(f\"{image_id}\\nInput image\")\n","        plt.axis('off')\n","\n","        fig, axes = plt.subplots()\n","        axes.imshow(image)\n","        show_mask(sf_seg, axes)\n","        axes.title.set_text(f\"IoU score = {iou_score:6.4f}\\nPredicted mask\")\n","        axes.axis(\"off\")\n","\n","        fig, axes = plt.subplots()\n","        axes.imshow(image)\n","        ground_truth_seg = np.array(mask)\n","        show_mask(ground_truth_seg, axes)\n","        axes.title.set_text(f\"Ground truth mask\")\n","        axes.axis(\"off\")"],"metadata":{"id":"w17nZtEsDhnH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["IoUãŒä½ã„ç”»åƒã‚’è¦‹ã¦ã¿ã‚‹ã¨ã€carã®é ˜åŸŸãŒå°ã•ã„å ´åˆã€ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒä¸Šæ‰‹ãã„ã£ã¦ã„ã‹ãªã„ã‚ˆã†ã§ã™\n","\n","ã“ã®IoUã®ä½ã„ç”»åƒã®å‚¾å‘ã¯ [SAMã‚’fine-tuningã—ãŸå ´åˆã®æ¤œè¨¼](https://github.com/yh0sh/MyStudy/blob/main/SAM/Fine_tune_SAM_(segment_anything)_on_CamVid.ipynb)ã¨åŒæ§˜ã§ã™\n","\n","ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã«ã¯ã€carã®é ˜åŸŸãŒå°ã•ã„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã“ã¨ã‚’è€ƒãˆã‚‹ã¨è‰¯ã•ãã†ã§ã™"],"metadata":{"id":"6MyqAtR3FW7S"}},{"cell_type":"markdown","source":["# ã¾ã¨ã‚\n","\n","CamVidãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€Hugging Faceã®SegFormerã‚’ç”¨ã„ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è©¦ã—ã¾ã—ãŸ\n","\n","- å‰å‡¦ç†ã¨ã—ã¦ã€ç”»åƒã‚’256x256ã«ãƒªã‚µã‚¤ã‚ºã€maskãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã¯carã®é ˜åŸŸã‚’1ã€ãã‚Œä»¥å¤–ã‚’0ã¨å€¤ã‚’å¤‰ãˆã‚‹åŠ å·¥ã‚’è¡Œã„ã¾ã—ãŸ\n","    - ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯Hugging Faceã®datasetsã«ç™»éŒ²ã—ã¦ãŠã‚Šã€èª°ã§ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ©ç”¨ã§ãã¾ã™\n","\n","- äº‹å‰å­¦ç¿’ã¨ã—ã¦ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚\n","ä»Šå›ã¯ã€Œcarã€ã®ã¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ãŸã‚ã€äºŒå€¤ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã„ã¾ã—ãŸã€‚\n","\n","- fine-tuningã—ãŸSegFormerãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã‚‹ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯91%ç¨‹åº¦ã€è©•ä¾¡ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã¯75%ç¨‹åº¦ã¨é«˜ç²¾åº¦ãªIoUã¨ãªã‚Šã¾ã—ãŸã€‚\n","\n","   - å‰å›fine-tuningã—ãŸSAMãƒ¢ãƒ‡ãƒ«ã«æ¯”ã¹ã¦ã€è©•ä¾¡ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯3-5%ç¨‹åº¦ã€ä½ã„å¹³å‡å€¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚\n"],"metadata":{"id":"hL4aI3TjjjWf"}}]}