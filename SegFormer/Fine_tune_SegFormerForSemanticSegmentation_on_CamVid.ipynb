{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Notebook: SegFormer for Semantic Segmentation on CamVid\n","\n","このノートブックでは、CamVidデータセットを使って、Hugging faceにあるSegFormerを使ったセグメンテーションを試します\n","\n","ノートブック作成にあたり、下記を参考にさせて頂きました🤗\n","\n","- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n","- https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb\n"],"metadata":{"id":"-wGmNpuJhUA9"}},{"cell_type":"markdown","source":["# セットアップ\n","fine-tuneしたモデルを保存するgoogle driveをマウントします\n","\n","Hugging faceのモジュール(Transformers, Datasets)をインストールします"],"metadata":{"id":"HckN1MHDjT27"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install transformers\n","!pip install datasets"],"metadata":{"id":"R3lBRK5D5Xur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# データセットのロード\n","以前、[SAMのfine-tuningを使ったセグメンテーションの検証](https://github.com/yh0sh/MyStudy/blob/main/SAM/Fine_tune_SAM_(segment_anything)_on_CamVid.ipynb)で作成したCamVidを修正した画像のデータセットをHugging faceのdatasetsに登録しました\n","\n","https://huggingface.co/datasets/yh0sh/resized_camvid_annot_car\n","\n","- オリジナルの画像とmaskデータを320\\*480 => 256\\*256へリサイズ\n","- maskデータはcarの領域は1、car以外の領域は0になるように加工\n","\n","今回はこのデータセットを使用します"],"metadata":{"id":"-psRNzbWbnPW"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"yh0sh/resized_camvid_annot_car\")"],"metadata":{"id":"0uqUHrzh47UA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def show_mask(mask, ax, random_color=False):\n","    if random_color:\n","        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n","    else:\n","        color = np.array([30/255, 144/255, 255/255, 0.6])\n","    h, w = mask.shape[-2:]\n","    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n","    ax.imshow(mask_image)\n","\n","sample = dataset[\"train\"][10]\n","\n","# 画像データ\n","image = sample[\"image\"]\n","\n","# maskデータ\n","# NOTE maskデータはgrayscaleに変換後、最大値で割ることで{0, 1}の値域に標準化する\n","mask = np.array(sample[\"annot\"].convert(\"L\")).astype(\"float64\")\n","mask /= np.nanmax(mask)\n","\n","fig, axes = plt.subplots()\n","axes.imshow(np.array(image))\n","show_mask(np.array(mask), axes)\n","axes.title.set_text(f\"Ground truth mask\")\n","axes.axis(\"off\")\n","plt.show()"],"metadata":{"id":"n1LA3ywS7qpL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PyTorch Datasetの作成\n","PyTorch Datasetのカスタムクラスを作ります"],"metadata":{"id":"WboY52-gs-4V"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import os\n","from PIL import Image\n","\n","class SemanticSegmentationDataset(Dataset):\n","\n","    def __init__(self, dataset, feature_extractor, data_label):\n","        assert data_label in [\"train\", \"validation\", \"test\"]\n","        self.dataset = dataset[data_label]\n","        self.feature_extractor = feature_extractor\n","        self.data_label = data_label\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","\n","        image = self.dataset[idx][\"image\"]\n","\n","        # NOTE maskデータはgrayscaleに変換後、最大値で割ることで{0, 1}の値域に標準化する\n","        mask = np.array(self.dataset[idx][\"annot\"].convert(\"L\")).astype(\"float64\")\n","        mask /= np.nanmax(mask)\n","        segmentation_map = mask\n","\n","        # randomly crop + pad both image and segmentation map to same size\n","        encoded_inputs = self.feature_extractor(image, segmentation_map,\n","                                                return_tensors=\"pt\")#, input_data_format=\"channels_last\")\n","\n","        for k,v in encoded_inputs.items():\n","          encoded_inputs[k].squeeze_() # remove batch dimension\n","\n","        return encoded_inputs"],"metadata":{"id":"5K0yxwjIs0nz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FeatureExtractorの読み込み\n","SegFormerのFeatureExtractorを読み込みます"],"metadata":{"id":"xZwn-1WRxNiP"}},{"cell_type":"code","source":["from transformers import SegformerImageProcessor\n","\n","feature_extractor = SegformerImageProcessor(do_reduce_labels=False) # 今回は2値分類なので do_reduce_labels = False とすることで、\n","                                                                    # unknownは除外せず、label = 0として使用\n","\n","train_dataset = SemanticSegmentationDataset(dataset, feature_extractor, \"train\")\n","valid_dataset = SemanticSegmentationDataset(dataset, feature_extractor, \"validation\")\n","test_dataset = SemanticSegmentationDataset(dataset, feature_extractor, \"test\")\n","\n","print(\"Number of training examples:\", len(train_dataset))\n","print(\"Number of validation examples:\", len(valid_dataset))\n","print(\"Number of testing examples:\", len(test_dataset))"],"metadata":{"id":"8EhOpGYJxdFx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PyTorch DataLoaderの作成\n","先に作成したPyTorch Datasetを元にDataLoaderを作成します"],"metadata":{"id":"Km4-gFqjyNDY"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from collections import Counter\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","# SegFormerImageProcessorにより、256*256 => 512*512にリサイズ\n","for batch in train_dataloader:\n","    for k, v in batch.items():\n","        print (k, np.array(v).shape)\n","\n","    # carにセグメントされたピクセルサイズの確認\n","    # label = 0 (= unknown)をmaskしてプロット\n","    mask = (batch[\"labels\"] != 0)[0]\n","\n","    plt.figure()\n","    plt.imshow(mask, cmap=\"gray\")\n","    break"],"metadata":{"id":"yZRw7cEGyO_V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelの読み込み"],"metadata":{"id":"koBNkTcWk9ae"}},{"cell_type":"markdown","source":["SegFormerの事前学習としてラベルリストを作成します\n","\n","今回は、carのみセグメンテーションするため、二値データとして扱います\n","\n","前処理として、以下を行いました(※)\n","\n"," - 2値問題として扱い、label = 0 (unknown), 1 (car) とする\n","      - ADE20Kのラベルリストを使用する場合は、huggingfaceのdatasetsにある[label-files](https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json)をダウンロードする\n"," - 0: unknownは無視せず学習対象として扱う\n","      - [mean_iou](https://huggingface.co/spaces/evaluate-metric/mean_iou)を用いて評価する際、ignore_indexにunknownのラベルを指定することで評価から除外することが出来る\n","      - しかし、unknownを無視すると、carのラベルのついた領域(ground truth)内でしか評価しなくなるため、学習ではlossを正しく計算できなくなる\n"," - 0: unknownを255としない\n","      - do_reduce_labels = Trueとしない\n","\n","※ 何度か値やパラメータを変えて実施した上での調査結果となりますが、解釈が間違っている可能性があります\n","\n","誤解しているところがあれば、ご指摘頂けると助かります🙏"],"metadata":{"id":"rSIJO6BNI5B4"}},{"cell_type":"code","source":["from transformers import SegformerForSemanticSegmentation\n","\n","# NOTE: ADE20Kで使用されているlabelリストを使用する場合は、以下のコメントアウトを外す\n","#import json\n","#from huggingface_hub import cached_download, hf_hub_url, hf_hub_download\n","#\n","## load id2label mapping from a JSON on the hub\n","#repo_id = \"huggingface/label-files\"\n","#filename = \"ade20k-id2label.json\"\n","#id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type=\"dataset\")), \"r\"))\n","#id2label = {int(k): v for k, v in id2label.items()}\n","#label2id = {v: k for k, v in id2label.items()}\n","\n","id2label = {0: \"unknown\", 1: \"car\"}\n","label2id = {v: k for k, v in id2label.items()}\n","\n","# define model\n","model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n","                                                         num_labels=2,\n","                                                         id2label=id2label,\n","                                                         label2id=label2id,\n",")"],"metadata":{"id":"lIxsJ4GZk7_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# モデル学習"],"metadata":{"id":"kEn8lb1LoWgt"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score\n","from tqdm.notebook import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","\n","from collections import Counter\n","\n","num_epochs = 100\n","\n","# define optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n","# move model to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = \"cpu\"\n","model.to(device)\n","\n","# 学習結果を保存する\n","save_dir = \"/content/drive/MyDrive/colab/SegFormer\"   # 所望の場所に変更して下さい\n","losses_path = f\"{save_dir}/loss_ft_segformer_camvid-b0.csv\"\n","columns = [\"epoch\", \"loss_train\", \"loss_eval\"]\n","df_loss = pd.DataFrame([], columns=columns)\n","start_epoch = 0\n","best_loss = np.inf\n","\n","# 途中から学習を再開する場合、保存したチェックポイントを読み込む\n","save_path = f\"{save_dir}/fine_tune_segformer_camvid-b0.pth\"\n","if os.path.exists(save_path):\n","    if device == \"cpu\":\n","        checkpoint = torch.load(save_path, map_location=torch.device(\"cpu\"))\n","    else:\n","        checkpoint = torch.load(save_path)\n","    model.load_state_dict(checkpoint)\n","\n","    df_loss = pd.read_csv(losses_path)\n","    start_epoch = df_loss[\"epoch\"].values[-1] + 1\n","    best_loss = df_loss[\"loss_eval\"].min()\n","\n","for epoch in range(start_epoch, num_epochs):  # loop over the dataset multiple times\n","    print(f'## EPOCH: {epoch} best_loss_eval: {best_loss}')\n","\n","    # 学習フェーズ\n","    model.train()\n","    epoch_losses = []\n","\n","    print (f\"train data {len(train_dataloader)}...\")\n","    for idx, batch in enumerate(tqdm(train_dataloader)):\n","        # get the inputs;\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(pixel_values=pixel_values, labels=labels)\n","        loss, logits = outputs.loss, outputs.logits\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_losses.append(loss.item())\n","    loss_train = np.mean(epoch_losses)\n","    print(f'Mean loss (train): {loss_train}')\n","\n","    # 評価フェーズ\n","    model.eval()\n","    with torch.no_grad():\n","        epoch_losses = []\n","\n","        print (f\"valid data {len(valid_dataloader)}...\")\n","        for idx, batch in enumerate(tqdm(valid_dataloader)):\n","            # get the inputs;\n","            pixel_values = batch[\"pixel_values\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            # forward + backward + optimize\n","            outputs = model(pixel_values=pixel_values, labels=labels)\n","            loss, logits = outputs.loss, outputs.logits\n","\n","            epoch_losses.append(loss.item())\n","\n","        loss_eval = np.mean(epoch_losses)\n","        print(f'Mean loss (valid): {loss_eval}')\n","\n","        # 学習結果保存\n","        result = {\n","            \"epoch\": epoch,\n","            \"loss_train\": loss_train,\n","            \"loss_eval\": loss_eval\n","        }\n","        df_loss = pd.concat([df_loss, pd.DataFrame([result])], axis=0)\n","        df_loss.to_csv(losses_path, index=False)\n","\n","        # 評価値更新\n","        if np.mean(epoch_losses) < best_loss:\n","            best_loss = np.mean(epoch_losses)\n","            torch.save(model.state_dict(), save_path)\n","            print(\"Model saved!\")\n"],"metadata":{"id":"L2KWD3COoV80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 学習結果の可視化"],"metadata":{"id":"3b2UEKQhIlAM"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure()\n","plt.plot(df_loss[\"epoch\"], df_loss[\"loss_train\"], label=\"train\")\n","plt.plot(df_loss[\"epoch\"], df_loss[\"loss_eval\"], label=\"eval\")\n","plt.title(\"Learning result\")\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","plt.grid(True)\n","plt.legend(loc=\"best\")\n","plt.show()"],"metadata":{"id":"ss9s3LBAInN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["epoch数が増えるに連れて損失は低下しており、学習は順調に行われたようです"],"metadata":{"id":"-A89l_G6KZlx"}},{"cell_type":"markdown","source":["# 推定結果\n","学習に使用したデータを使い、学習が正しく行われたか確認します\n","\n","予測マスク値、正解マスク値を確認します"],"metadata":{"id":"gB-pHDduIxmN"}},{"cell_type":"code","source":["# 評価用に乱数を固定する\n","seed = 42\n","torch.manual_seed(seed)\n","\n","# GPUを使う場合には以下のコードも追加\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","idx = 10\n","data = dataset[\"train\"][idx]\n","image = data[\"image\"]\n","\n","mask = np.array(sample[\"annot\"].convert(\"L\")).astype(\"float64\")\n","mask /= np.nanmax(mask)\n","\n","encoded_inputs = feature_extractor(image, return_tensors=\"pt\")#, input_data_format=\"channels_last\")\n","pixel_values = encoded_inputs.pixel_values.to(device)\n","\n","model.eval()\n","\n","# forward pass\n","with torch.no_grad():\n","    outputs = model(pixel_values=pixel_values)\n","\n","# logits are of shape (batch_size, num_labels, height/4, width/4)\n","logits = outputs.logits.cpu()\n","print(logits.shape)\n","\n","# First, rescale logits to original image size\n","upsampled_logits = nn.functional.interpolate(logits,\n","                size=image.size[::-1], # (height, width)\n","                mode='bilinear',\n","                align_corners=False)\n","\n","seg = upsampled_logits.argmax(dim=1)[0]\n","\n","sf_seg = seg.detach().cpu().numpy()\n","\n","fig, axes = plt.subplots()\n","axes.imshow(np.array(image))\n","show_mask(np.array(sf_seg), axes)\n","axes.title.set_text(f\"Predicted mask\")\n","axes.axis(\"off\")\n","plt.show()\n","\n","fig, axes = plt.subplots()\n","axes.imshow(np.array(image))\n","ground_truth_seg = np.array(mask)\n","show_mask(ground_truth_seg, axes)\n","axes.title.set_text(f\"Ground truth mask\")\n","axes.axis(\"off\")"],"metadata":{"id":"LhRT3xItIxNI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上記結果を元にIoUを計算します"],"metadata":{"id":"gglpJsHQ9QF2"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_iou(segmentation1, segmentation2):\n","    intersection = np.logical_and(segmentation1, segmentation2)\n","    union = np.logical_or(segmentation1, segmentation2)\n","    iou = np.sum(intersection) / np.sum(union)\n","    return iou\n","\n","# IoUを計算\n","iou_score = calculate_iou(sf_seg.astype(bool), ground_truth_seg.astype(bool))\n","print(f\"IoU: {iou_score}\")"],"metadata":{"id":"qGL0RiV49PIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["次に使用したデータ全てのIoUを計算し、学習、評価、テストデータのIoU平均値をそれぞれ確認します"],"metadata":{"id":"toC7kIJf9YVD"}},{"cell_type":"code","source":["# 評価用に乱数を固定する\n","seed = 42\n","torch.manual_seed(seed)\n","\n","# GPUを使う場合には以下のコードも追加\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","labels = [\"train\", \"validation\", \"test\"]\n","iou_scores_dict = {label: [] for label in labels}\n","\n","iou_scores_path = \"{save_dir}/iou_scores_{label}_ft_sf_camvid-b0.csv\"\n","for label in labels:\n","    if os.path.exists(iou_scores_path.format(save_dir = save_dir, label = label)):\n","        continue\n","\n","    print (f\"###  {label}\")\n","    for idx, inputs in tqdm(enumerate(dataset[label]), desc=f\"{label} data {len(dataset[label])}\"):\n","\n","        image_id = inputs[\"image_id\"]\n","        image = inputs[\"image\"]\n","\n","        mask = np.array(inputs[\"annot\"].convert(\"L\")).astype(\"float64\")\n","        mask /= np.nanmax(mask)\n","\n","        encoded_inputs = feature_extractor(image, return_tensors=\"pt\")#, input_data_format=\"channels_last\")\n","        pixel_values = encoded_inputs.pixel_values.to(device)\n","\n","        model.eval()\n","\n","        # forward pass\n","        with torch.no_grad():\n","            outputs = model(pixel_values=pixel_values)\n","\n","        # logits are of shape (batch_size, num_labels, height/4, width/4)\n","        logits = outputs.logits.cpu()\n","\n","        # First, rescale logits to original image size\n","        upsampled_logits = nn.functional.interpolate(logits,\n","                        size=image.size[::-1], # (height, width)\n","                        mode='bilinear',\n","                        align_corners=False)\n","\n","        seg = upsampled_logits.argmax(dim=1)[0]\n","\n","        sf_seg = seg.detach().cpu().numpy()\n","\n","        ground_truth_seg = np.array(mask)\n","\n","        iou_score = calculate_iou(sf_seg.astype(bool), ground_truth_seg.astype(bool))\n","        iou_scores_dict[label].append([image_id, iou_score])\n","    print (f\"mean: {np.nanmean(np.array(iou_scores_dict[label])[:,1].astype(float), axis=0)}\")\n","    print (f\"std: {np.nanstd(np.array(iou_scores_dict[label])[:,1].astype(float), axis=0)}\")\n","\n","    data = np.array(iou_scores_dict[label])\n","    pd.DataFrame(data).to_csv(iou_scores_path.format(save_dir = save_dir, label = label),\n","                              header=None, index=False)"],"metadata":{"id":"sMByJVuC9ZUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["means = []\n","stds = []\n","nums = []\n","for label in labels:\n","    df = pd.read_csv(iou_scores_path.format(save_dir = save_dir, label = label), header=None)\n","    means.append(df.values[:,1].mean())\n","    stds.append(df.values[:,1].std())\n","    nums.append(df.values[:,1].size)\n","\n","plt.figure()\n","plt.errorbar(np.arange(len(labels)), means, yerr=stds)\n","plt.xticks(np.arange(len(labels)), [f\"{label} (N={num})\" for label, num in zip(labels, nums)])\n","plt.ylabel(\"IoU\")\n","plt.title(\"IoU Score\")\n","plt.grid(True)"],"metadata":{"id":"sYgJdg3j_EYE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["学習データは91%程度、評価とテストデータでは75%程度と中々の高精度となりました\n","\n","これは [SAMをfine-tuningした結果](https://github.com/yh0sh/MyStudy/blob/main/SAM/Fine_tune_SAM_(segment_anything)_on_CamVid.ipynb)に比べて、評価とテストデータは3-5%程度低い平均値となっています\n","\n","また、標準偏差は5%程度大きくなっており、データ単位での精度の差が大きくなっています\n","\n","最高と最低のIoUスコアとなった画像を出力して、入力画像の特徴や傾向を目視確認して精度への影響を確認します"],"metadata":{"id":"PJto0TWfBgST"}},{"cell_type":"code","source":["# 評価用に乱数を固定する\n","seed = 42\n","torch.manual_seed(seed)\n","\n","# GPUを使う場合には以下のコードも追加\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","def show_box(box, ax):\n","    x0, y0 = box[0], box[1]\n","    w, h = box[2] - box[0], box[3] - box[1]\n","    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n","\n","from collections import Counter\n","\n","for label in labels:\n","    df = pd.read_csv(iou_scores_path.format(save_dir = save_dir, label = label), header=None)\n","    image_ids = df.values[:,0]\n","    iou_scores = df.values[:,1]\n","\n","    max_score_idx = np.argmax(iou_scores)\n","    min_score_idx = np.argmin(iou_scores)\n","\n","    ds_dict = {inputs[\"image_id\"]: inputs for inputs in dataset[label]}\n","    for score_idx in [max_score_idx, min_score_idx]:\n","        iou_score = iou_scores[score_idx]\n","        image_id = image_ids[score_idx]\n","\n","        inputs = ds_dict[image_id]\n","\n","        image = inputs[\"image\"]\n","\n","        mask = np.array(inputs[\"annot\"].convert(\"L\")).astype(\"float64\")\n","        mask /= np.nanmax(mask)\n","\n","        encoded_inputs = feature_extractor(image, return_tensors=\"pt\")#, input_data_format=\"channels_last\")\n","        pixel_values = encoded_inputs.pixel_values.to(device)\n","\n","        model.eval()\n","\n","        # forward pass\n","        with torch.no_grad():\n","            outputs = model(pixel_values=pixel_values)\n","\n","        # logits are of shape (batch_size, num_labels, height/4, width/4)\n","        logits = outputs.logits.cpu()\n","\n","        # First, rescale logits to original image size\n","        upsampled_logits = nn.functional.interpolate(logits,\n","                        size=image.size[::-1], # (height, width)\n","                        mode='bilinear',\n","                        align_corners=False)\n","\n","        seg = upsampled_logits.argmax(dim=1)[0]\n","\n","        sf_seg = seg.detach().cpu().numpy()\n","\n","        ground_truth_seg = np.array(mask)\n","\n","        plt.figure()\n","        plt.imshow(image)\n","        plt.title(f\"{image_id}\\nInput image\")\n","        plt.axis('off')\n","\n","        fig, axes = plt.subplots()\n","        axes.imshow(image)\n","        show_mask(sf_seg, axes)\n","        axes.title.set_text(f\"IoU score = {iou_score:6.4f}\\nPredicted mask\")\n","        axes.axis(\"off\")\n","\n","        fig, axes = plt.subplots()\n","        axes.imshow(image)\n","        ground_truth_seg = np.array(mask)\n","        show_mask(ground_truth_seg, axes)\n","        axes.title.set_text(f\"Ground truth mask\")\n","        axes.axis(\"off\")"],"metadata":{"id":"w17nZtEsDhnH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["IoUが低い画像を見てみると、carの領域が小さい場合、セグメンテーションが上手くいっていかないようです\n","\n","このIoUの低い画像の傾向は [SAMをfine-tuningした場合の検証](https://github.com/yh0sh/MyStudy/blob/main/SAM/Fine_tune_SAM_(segment_anything)_on_CamVid.ipynb)と同様です\n","\n","精度を上げるためには、carの領域が小さい学習データを増やすことを考えると良さそうです"],"metadata":{"id":"6MyqAtR3FW7S"}},{"cell_type":"markdown","source":["# まとめ\n","\n","CamVidデータセットを使用し、Hugging FaceのSegFormerを用いたセグメンテーションを試しました\n","\n","- 前処理として、画像を256x256にリサイズ、maskデータについてはcarの領域を1、それ以外を0と値を変える加工を行いました\n","    - このデータセットはHugging Faceのdatasetsに登録しており、誰でもダウンロードして利用できます\n","\n","- 事前学習としてラベルリストを作成しました。\n","今回は「car」のみをセグメンテーションするため、二値データとして扱いました。\n","\n","- fine-tuningしたSegFormerモデルを用いると学習データは91%程度、評価とテストデータでは75%程度と高精度なIoUとなりました。\n","\n","   - 前回fine-tuningしたSAMモデルに比べて、評価とテストデータは3-5%程度、低い平均値となっています。\n"],"metadata":{"id":"hL4aI3TjjjWf"}}]}